{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc7676c-9a7a-412c-b94b-c1291b01eccb",
   "metadata": {},
   "source": [
    "## Load original data and test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef78f3af-0bf9-4ae5-9254-95ff4faf9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read STSbenchmark test dataset\n",
      "Original model performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8751906370474595"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, evaluation, models, InputExample\n",
    "import logging\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Model for which we apply dimensionality reduction\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# New size for the embeddings\n",
    "new_dimension = 128\n",
    "\n",
    "# We use AllNLI as a source of sentences to compute PCA\n",
    "nli_dataset_path = \"datasets/AllNLI.tsv.gz\"\n",
    "\n",
    "# We use the STS benchmark dataset to see how much performance we loose by using the dimensionality reduction\n",
    "sts_dataset_path = \"datasets/stsbenchmark.tsv.gz\"\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/AllNLI.tsv.gz\", nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/stsbenchmark.tsv.gz\", sts_dataset_path)\n",
    "\n",
    "\n",
    "# We measure the performance of the original model\n",
    "print(\"Read STSbenchmark test dataset\")\n",
    "eval_examples = []\n",
    "with gzip.open(sts_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"test\":\n",
    "            score = float(row[\"score\"]) / 5.0  # Normalize score to range 0 ... 1\n",
    "            eval_examples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))\n",
    "\n",
    "# Evaluate the original model on the STS benchmark dataset\n",
    "stsb_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name=\"sts-benchmark-test\")\n",
    "\n",
    "print(\"Original model performance:\")\n",
    "stsb_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d39ea6-57c7-4b1b-b501-b7b1f6ce1dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb34bd3-8a01-4ff5-8a51-8cf8e32e6e6a",
   "metadata": {},
   "source": [
    "## Optimized and Quantized model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28e2fe3-d66a-49fc-b185-d58aeb116c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "The ONNX file model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from optimum.onnxruntime import (\n",
    "    ORTModelForFeatureExtraction\n",
    ")\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5_optimized_quantized', model_max_length=512)\n",
    "\n",
    "optimized_quantized_model = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-large-en-v1.5_optimized_quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55163a6e-00ea-445c-bab7-5e08fdfd1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Quantized model performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8779827351465398"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Optimized Quantized model performance:\")\n",
    "stsb_evaluator(optimized_quantized_model, tokenizer = tokenizer, optimized_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68f2f7f-fe18-4126-baf5-78fcac0fddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in accuracy: -0.279209809908032\n"
     ]
    }
   ],
   "source": [
    "print('Loss in accuracy: {}'.format((0.8751906370474595 - 0.8779827351465398)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bec698-f964-4e5a-8f11-6138b82e9019",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction on original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b31efa4-d9fc-4424-9f97-eaa9e2b31e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 128 dimensions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8518175600685338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Reduce the embedding dimensions ########\n",
    "\n",
    "# Read sentences from NLI dataset\n",
    "nli_sentences = set()\n",
    "with gzip.open(nli_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        nli_sentences.add(row[\"sentence1\"])\n",
    "        nli_sentences.add(row[\"sentence2\"])\n",
    "\n",
    "nli_sentences = list(nli_sentences)\n",
    "random.shuffle(nli_sentences)\n",
    "\n",
    "# To determine the PCA matrix, we need some example sentence embeddings.\n",
    "# Here, we compute the embeddings for 20k random sentences from the AllNLI dataset\n",
    "pca_train_sentences = nli_sentences[0:20000]\n",
    "train_embeddings = model.encode(pca_train_sentences, convert_to_numpy=True)\n",
    "\n",
    "# Compute PCA on the train embeddings matrix\n",
    "pca = PCA(n_components=new_dimension)\n",
    "pca.fit(train_embeddings)\n",
    "pca_comp = np.asarray(pca.components_)\n",
    "\n",
    "# We add a dense layer to the model, so that it will produce directly embeddings with the new size\n",
    "dense = models.Dense(\n",
    "    in_features=model.get_sentence_embedding_dimension(),\n",
    "    out_features=new_dimension,\n",
    "    bias=False,\n",
    "    activation_function=torch.nn.Identity(),\n",
    ")\n",
    "dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n",
    "model.add_module(\"dense\", dense)\n",
    "# Evaluate the model with the reduce embedding size\n",
    "print(\"Model with {} dimensions:\".format(new_dimension))\n",
    "stsb_evaluator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ddfd22-2a47-485b-8b37-96435603eccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in Accuracy:2.6165175078005976\n"
     ]
    }
   ],
   "source": [
    "print('Loss in Accuracy:{}'.format((0.8779827351465398 - 0.8518175600685338)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74bbe8f-8446-4b62-b134-8f9210ea730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_transformers.SentenceTransformer.SentenceTransformer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6bd308-4133-4ecd-8a04-cf43833c3ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimum.onnxruntime.modeling_ort.ORTModelForFeatureExtraction"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a203451-febf-45cb-ba44-5a0a0d8ae454",
   "metadata": {},
   "source": [
    "# Save and Load the model to test for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48221c-8aa5-49ee-99b6-3230664c9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bge-large-en-v1.5_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06f70c-2538-4f62-a190-951e1b7624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_128 = SentenceTransformer('bge-large-en-v1.5_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56428b8-f715-4456-826a-08eb103a96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_128 = model_128.encode('encode this text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc8a6d-0202-471c-b763-4549f7d02650",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027035b8-cef5-4511-bfee-3a8ba71d3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emb_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87693b53-0147-47a0-9a72-9c978bf1568a",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction on optimized quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ddf205-6a3f-427c-a5dc-cfb6f7fbce4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ORTModelForFeatureExtraction' object has no attribute 'add_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimized_quantized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_module\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m, dense)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ORTModelForFeatureExtraction' object has no attribute 'add_module'"
     ]
    }
   ],
   "source": [
    "optimized_quantized_model.add_module(\"dense\", dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077db389-29fe-4772-adcc-28a38bb61159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized Quantized Model with {} dimensions:\".format(new_dimension))\n",
    "stsb_evaluator(optimized_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed02daf-35c1-421e-9d0d-7b9a77c3afdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
