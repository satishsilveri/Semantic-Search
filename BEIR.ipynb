{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af668293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from beir.retrieval import models\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "class EvaluateSBERTModels:\n",
    "    \n",
    "    def __init__(self, config_file_path):\n",
    "        \n",
    "        with open(config_file_path) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        self.target_dir = self.config['target_dir']\n",
    "        \n",
    "        if self.target_dir is None or len(self.target_dir) == 0:\n",
    "            self.target_dir = os.getcwd()\n",
    "            \n",
    "        \n",
    "        # evaluator inputs path\n",
    "        self.eval_input_base_path = os.path.join(self.target_dir,'sbert_eval_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")),'evaluator_input')\n",
    "        if not os.path.exists(self.eval_input_base_path):\n",
    "            os.makedirs(self.eval_input_base_path)\n",
    "        \n",
    "        # create results base path\n",
    "        self.results_base_path = os.path.join(self.target_dir,'sbert_eval_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")),'results')\n",
    "        if not os.path.exists(self.results_base_path):\n",
    "            os.makedirs(self.results_base_path)\n",
    "    \n",
    "    def create_data_for_evaluator(self):\n",
    "        '''\n",
    "        Function to convert input data to BEIR data loader compatible format.\n",
    "        '''\n",
    "        \n",
    "        assert len(self.config['data_path'])>0, \"Data path cannot be empty.\"\n",
    "        \n",
    "        # load data\n",
    "        data = pd.read_csv(self.config['data_path'])\n",
    "\n",
    "        corpus=[]\n",
    "        queries=[]\n",
    "        qrels=[]\n",
    "\n",
    "        for index,item in data.iterrows():\n",
    "            data={}\n",
    "            query={}\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            data['_id'] = doc_id\n",
    "            data['text'] = item['Answers']\n",
    "            data['title'] = \"\"\n",
    "            corpus.append(data)\n",
    "            q_id = str(uuid.uuid4())\n",
    "            query['_id'] = q_id\n",
    "            query['text'] = item['Questions']\n",
    "            queries.append(query)\n",
    "            qrels.append('{}\\t{}\\t1'.format(q_id, doc_id))\n",
    "        \n",
    "        \n",
    "        # write corpus\n",
    "        with open(os.path.join(self.eval_input_base_path,'corpus.jsonl'),'w') as f:\n",
    "            for index,_dict in enumerate(corpus):\n",
    "                if index<len(corpus)-1:\n",
    "                    f.write(json.dumps(_dict)+'\\n')\n",
    "                else:\n",
    "                    f.write(json.dumps(_dict))\n",
    "                    \n",
    "        # write queries\n",
    "        with open(os.path.join(self.eval_input_base_path,'queries.jsonl'),'w') as f:\n",
    "            for index,_dict in enumerate(queries):\n",
    "                if index<len(corpus)-1:\n",
    "                    f.write(json.dumps(_dict)+'\\n')\n",
    "                else:\n",
    "                    f.write(json.dumps(_dict))\n",
    "                    \n",
    "        # write qrels\n",
    "        with open(os.path.join(self.eval_input_base_path,'qrels.tsv'),'w') as f:\n",
    "            # add header\n",
    "            f.write('query-id\\tcorpus-id\\tscore')\n",
    "            for index,line in enumerate(qrels):\n",
    "                if index<len(corpus)-1:\n",
    "                    f.write(line+\"\\n\")\n",
    "                else:\n",
    "                    f.write(line)\n",
    "    \n",
    "    def load_data_for_evaluator(self):\n",
    "        '''\n",
    "        Function to load the data for the evaluator.\n",
    "        '''\n",
    "        corpus, queries, qrels = GenericDataLoader(\n",
    "        corpus_file=os.path.join(self.eval_input_base_path,'corpus.jsonl'), \n",
    "        query_file=os.path.join(self.eval_input_base_path,'queries.jsonl'), \n",
    "        qrels_file=os.path.join(self.eval_input_base_path,'qrels.tsv')).load_custom()\n",
    "        \n",
    "        return corpus, queries, qrels\n",
    "        \n",
    "    \n",
    "    def evaluate_model(self, model, corpus, queries, qrels):\n",
    "        '''\n",
    "        Function to evaluate a SBERT model.\n",
    "\n",
    "        Input:\n",
    "            model: model_path or model id.\n",
    "            batch_size: batch size for input.\n",
    "            score_function: distance measure ('dot' or 'cos_sim')\n",
    "        Output:\n",
    "            ndgc: Normalized Discounted cumulative gain scores for a given model.\n",
    "            _map: Mean average precision scores for a given model.\n",
    "            recall: Recall scores for a given model.\n",
    "            precision: Precision scores for a given model.\n",
    "        '''\n",
    "        \n",
    "        batch_size = self.config['batch_size']\n",
    "        if batch_size is None:\n",
    "            batch_size = 64\n",
    "            \n",
    "        score_function = self.config['score_function']\n",
    "        if score_function is None:\n",
    "            score_function = \"dot\"\n",
    "            \n",
    "        \n",
    "        model = DRES(models.SentenceBERT(model), batch_size=batch_size)\n",
    "        retriever = EvaluateRetrieval(model, score_function=score_function)\n",
    "        results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "        ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "\n",
    "        return ndcg, _map, recall, precision\n",
    "\n",
    "\n",
    "    def run_evaluator(self):\n",
    "        '''\n",
    "        Pipeline function to run the evaluator.\n",
    "        '''\n",
    "        \n",
    "        assert len(self.config['models'])>0, \"Evaluator requires 1 or more models.\"\n",
    "        \n",
    "        ndgc_results=[]\n",
    "        map_results=[]\n",
    "        recall_results=[]\n",
    "        precision_results=[]\n",
    "        \n",
    "        # create data for evalutor\n",
    "        self.create_data_for_evaluator()\n",
    "        \n",
    "        # load data for evaluator\n",
    "        corpus, queries, qrels = self.load_data_for_evaluator()\n",
    "        \n",
    "        # analyze models\n",
    "        for model in self.config['models']:\n",
    "            \n",
    "            ndcg, _map, recall, precision = self.evaluate_model(model, corpus, queries, qrels)\n",
    "            ndcg['Model'] = model\n",
    "            ndgc_results.append(ndcg)\n",
    "            _map['Model'] = model\n",
    "            map_results.append(_map)\n",
    "            recall['Model'] = model\n",
    "            recall_results.append(recall)\n",
    "            precision['Model'] = model\n",
    "            precision_results.append(precision)\n",
    "            \n",
    "        \n",
    "        with open(os.path.join(self.results_base_path,'NDGC.json'), 'w') as f:\n",
    "            f.write(json.dumps(ndgc_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'MAP.json'), 'w') as f:\n",
    "            f.write(json.dumps(map_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'RECALL.json'), 'w') as f:\n",
    "            f.write(json.dumps(recall_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'PRECISION.json'), 'w') as f:\n",
    "            f.write(json.dumps(precision_results))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0b6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_evaluator = EvaluateSBERTModels(config_file_path='config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9aae8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thenlper/gte-large', 'BAAI/bge-large-en-v1.5', 'intfloat/e5-large-v2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_evaluator.config['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2596af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc95a0f19e8488681a8973408015f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254cb0b2a7724cb8bc41727669b4a48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13805cc6343a4e8284dd630fa07c9306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43cfe05e4d040f7bb5eed011d59892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bec4f96b8f4419a0a8a1b215483b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d59fa0e2584d69a63a71b5e29312bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d768e1c5a4714a49bb42727398d1dc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbert_evaluator.run_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f67c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eb4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
