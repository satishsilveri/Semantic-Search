{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af668293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satishsilveri/anaconda3/lib/python3.11/site-packages/beir/datasets/data_loader.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from beir.retrieval import models\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import uuid\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "class EvaluateSBERTModels:\n",
    "    \n",
    "    def __init__(self, config_file_path):\n",
    "        \n",
    "        with open(config_file_path) as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        self.target_dir = self.config['target_dir']\n",
    "        \n",
    "        if self.target_dir is None or len(self.target_dir) == 0:\n",
    "            self.target_dir = os.getcwd()\n",
    "            \n",
    "        \n",
    "        # evaluator inputs path\n",
    "        self.eval_input_base_path = os.path.join(self.target_dir,'sbert_eval_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")),'evaluator_input')\n",
    "        if not os.path.exists(self.eval_input_base_path):\n",
    "            os.makedirs(self.eval_input_base_path)\n",
    "        \n",
    "        # create results base path\n",
    "        self.results_base_path = os.path.join(self.target_dir,'sbert_eval_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")),'results')\n",
    "        if not os.path.exists(self.results_base_path):\n",
    "            os.makedirs(self.results_base_path)\n",
    "            \n",
    "    def assign_ids(self, df):\n",
    "        '''\n",
    "        Function to assign unique ids for questions and answers.\n",
    "        '''\n",
    "        df['q_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "        df['ans_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_threshold(self, df, threshold_sample_fraction = 0.30):\n",
    "        '''\n",
    "        Dynamically compute threshold value based on embeddings.\n",
    "        \n",
    "        Formula:\n",
    "        \n",
    "        threshold = min(cosine_sim) + ((max(cosine_sim) - min(cosine_sim))/3)\n",
    "        \n",
    "        Inputs:\n",
    "            df: input dataframe with embeddings.\n",
    "            question_col: column name of the questions.\n",
    "            answer_col: column name of the answers.\n",
    "            threshold_sample_fraction: fraction of datapoints to consider for computing threshold.\n",
    "        Outputs:\n",
    "            threshold: computed threshold.\n",
    "        '''\n",
    "        \n",
    "        frac_df = df.sample(frac=threshold_sample_fraction)\n",
    "        \n",
    "        frac_df['cosine_sim'] = frac_df.apply(lambda row: self.compute_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\n",
    "        \n",
    "        similarity_scores = frac_df['cosine_sim'].tolist()\n",
    "        \n",
    "        min_score = min(similarity_scores)\n",
    "        max_score = max(similarity_scores)\n",
    "        \n",
    "        print('Min: {}'.format(min_score))\n",
    "        print('Max: {}'.format(max_score))\n",
    "        \n",
    "        threshold = min_score + ((max_score - min_score)/3)\n",
    "        \n",
    "        print('Threshold: {}'.format(threshold))\n",
    "        \n",
    "        return threshold\n",
    "        \n",
    "\n",
    "    def compute_cosine_similarity(self, embeddings1, embeddings2):\n",
    "        '''\n",
    "        Function to generate cosine similarity between 2 embeddings.\n",
    "        '''\n",
    "        return cosine_similarity(embeddings1.reshape(1, -1), embeddings2.reshape(1, -1))[0, 0]\n",
    "\n",
    "    def get_qrels(self, df, model_id, question_column, answer_column, negative_samples=False, negative_sample_size=10):\n",
    "        '''\n",
    "        Function that is used to generate qrels for evaluator input.\n",
    "        \n",
    "        Input:\n",
    "            df: input dataframe\n",
    "            model_id: model id for sentence transformers.\n",
    "            question_column: name of the column containing questions.\n",
    "            answer_column: name of the column containing answers.\n",
    "            negative_samples: flag to include negative samples.\n",
    "            threshold: threshold value for negative samples.\n",
    "            \n",
    "        Output:\n",
    "            qrels: Qrels for input to evalautor\n",
    "        '''\n",
    "        qrels=[]\n",
    "        \n",
    "        qrels.append('query-id\\tcorpus-id\\tscore')\n",
    "\n",
    "        model = SentenceTransformer(model_id)\n",
    "        \n",
    "        threshold = 0.0\n",
    "        \n",
    "        if negative_samples:\n",
    "            # Compute embeddings for questions and answers\n",
    "            df['question_embeddings'] = df[question_column].progress_apply(lambda x: model.encode(x))\n",
    "            df['answer_embeddings'] = df[answer_column].progress_apply(lambda x: model.encode(x))\n",
    "            \n",
    "            # dynamically compute threshold\n",
    "            threshold = self.compute_threshold(df = df, threshold_sample_fraction = self.config['threshold_sample_fraction'])\n",
    "            \n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            # Positive sample (answer to the question)\n",
    "            qrels.append('{}\\t{}\\t1'.format(row['q_id'], row['ans_id']))\n",
    "            \n",
    "            if negative_samples:\n",
    "                # Negative sample (random answer from another question)\n",
    "                candidate_negatives = df[df['q_id'] != row['q_id']].sample(negative_sample_size)\n",
    "                random_negative = candidate_negatives.sample(1).iloc[0]\n",
    "                neg_similarity = self.compute_cosine_similarity(row['answer_embeddings'], random_negative['answer_embeddings'])\n",
    "\n",
    "                while neg_similarity >= threshold:\n",
    "                    random_negative = candidate_negatives.sample(1).iloc[0]\n",
    "                    neg_similarity = self.compute_cosine_similarity(row['answer_embeddings'], random_negative['answer_embeddings'])\n",
    "                \n",
    "                qrels.append('{}\\t{}\\t0'.format(row['q_id'], random_negative['ans_id']))\n",
    "\n",
    "        return qrels\n",
    "    \n",
    "    def create_data_for_evaluator(self):\n",
    "        '''\n",
    "        Function to convert input data to BEIR data loader compatible format.\n",
    "        '''\n",
    "        \n",
    "        assert len(self.config['data_path'])>0, \"Data path cannot be empty.\"\n",
    "        \n",
    "        # load data\n",
    "        data_df = pd.read_csv(self.config['data_path'])\n",
    "        \n",
    "        # Assign unique ids to questions and answers\n",
    "        data_df = self.assign_ids(data_df)\n",
    "\n",
    "        corpus=[]\n",
    "        queries=[]\n",
    "\n",
    "        for index,item in data_df.iterrows():\n",
    "            data={}\n",
    "            query={}\n",
    "            data['_id'] = item['ans_id']\n",
    "            data['text'] = item[self.config['answer_column']]\n",
    "            data['title'] = \"\"\n",
    "            corpus.append(data)\n",
    "            query['_id'] = item['q_id']\n",
    "            query['text'] = item[self.config['question_column']]\n",
    "            queries.append(query)\n",
    "        \n",
    "        negative_sampler_model_id = None\n",
    "        \n",
    "        if self.config['negative_sampler_model_id'] == \"random\":\n",
    "            # select random model_id for generating embeddings\n",
    "            negative_sampler_model_id = random.choice(self.config['models'])\n",
    "        else:\n",
    "            negative_sampler_model_id = self.config['negative_sampler_model_id']\n",
    "        \n",
    "        qrels = self.get_qrels(df=data_df, model_id=negative_sampler_model_id, question_column = self.config['question_column'], answer_column=self.config['answer_column'], negative_samples=self.config['negative_samples'], negative_sample_size = self.config['negative_sample_size'])\n",
    "        \n",
    "        # write corpus\n",
    "        with open(os.path.join(self.eval_input_base_path,'corpus.jsonl'),'w') as f:\n",
    "            for index,_dict in enumerate(corpus):\n",
    "                if index<len(corpus)-1:\n",
    "                    f.write(json.dumps(_dict)+'\\n')\n",
    "                else:\n",
    "                    f.write(json.dumps(_dict))\n",
    "                    \n",
    "        # write queries\n",
    "        with open(os.path.join(self.eval_input_base_path,'queries.jsonl'),'w') as f:\n",
    "            for index,_dict in enumerate(queries):\n",
    "                if index<len(corpus)-1:\n",
    "                    f.write(json.dumps(_dict)+'\\n')\n",
    "                else:\n",
    "                    f.write(json.dumps(_dict))\n",
    "                    \n",
    "        # write qrels\n",
    "        qrels_str = '\\n'.join(qrels)\n",
    "        with open(os.path.join(self.eval_input_base_path,'qrels.tsv'),'w') as f:\n",
    "            f.write(qrels_str)\n",
    "            \n",
    "    \n",
    "    def load_data_for_evaluator(self):\n",
    "        '''\n",
    "        Function to load the data for the evaluator.\n",
    "        '''\n",
    "        corpus, queries, qrels = GenericDataLoader(\n",
    "        corpus_file=os.path.join(self.eval_input_base_path,'corpus.jsonl'), \n",
    "        query_file=os.path.join(self.eval_input_base_path,'queries.jsonl'), \n",
    "        qrels_file=os.path.join(self.eval_input_base_path,'qrels.tsv')).load_custom()\n",
    "        \n",
    "        return corpus, queries, qrels\n",
    "        \n",
    "    \n",
    "    def evaluate_model(self, model, corpus, queries, qrels):\n",
    "        '''\n",
    "        Function to evaluate a SBERT model.\n",
    "\n",
    "        Input:\n",
    "            model: model_path or model id.\n",
    "            batch_size: batch size for input.\n",
    "            score_function: distance measure ('dot' or 'cos_sim')\n",
    "        Output:\n",
    "            ndgc: Normalized Discounted cumulative gain scores for a given model.\n",
    "            _map: Mean average precision scores for a given model.\n",
    "            recall: Recall scores for a given model.\n",
    "            precision: Precision scores for a given model.\n",
    "        '''\n",
    "        \n",
    "        batch_size = self.config['batch_size']\n",
    "        if batch_size is None:\n",
    "            batch_size = 64\n",
    "            \n",
    "        score_function = self.config['score_function']\n",
    "        if score_function is None:\n",
    "            score_function = \"dot\"\n",
    "            \n",
    "        \n",
    "        model = DRES(models.SentenceBERT(model), batch_size=batch_size)\n",
    "        retriever = EvaluateRetrieval(model, score_function=score_function)\n",
    "        results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "        ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "\n",
    "        return ndcg, _map, recall, precision\n",
    "\n",
    "\n",
    "    def run_evaluator(self):\n",
    "        '''\n",
    "        Pipeline function to run the evaluator.\n",
    "        '''\n",
    "        \n",
    "        assert len(self.config['models'])>0, \"Evaluator requires 1 or more models.\"\n",
    "        \n",
    "        ndgc_results=[]\n",
    "        map_results=[]\n",
    "        recall_results=[]\n",
    "        precision_results=[]\n",
    "        \n",
    "        # create data for evalutor\n",
    "        self.create_data_for_evaluator()\n",
    "        \n",
    "        # load data for evaluator\n",
    "        corpus, queries, qrels = self.load_data_for_evaluator()\n",
    "        \n",
    "        # analyze models\n",
    "        for model in self.config['models']:\n",
    "            \n",
    "            ndcg, _map, recall, precision = self.evaluate_model(model, corpus, queries, qrels)\n",
    "            ndcg['Model'] = model\n",
    "            ndgc_results.append(ndcg)\n",
    "            _map['Model'] = model\n",
    "            map_results.append(_map)\n",
    "            recall['Model'] = model\n",
    "            recall_results.append(recall)\n",
    "            precision['Model'] = model\n",
    "            precision_results.append(precision)\n",
    "            \n",
    "        \n",
    "        with open(os.path.join(self.results_base_path,'NDGC.json'), 'w') as f:\n",
    "            f.write(json.dumps(ndgc_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'MAP.json'), 'w') as f:\n",
    "            f.write(json.dumps(map_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'RECALL.json'), 'w') as f:\n",
    "            f.write(json.dumps(recall_results))\n",
    "            \n",
    "        with open(os.path.join(self.results_base_path,'PRECISION.json'), 'w') as f:\n",
    "            f.write(json.dumps(precision_results))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_evaluator = EvaluateSBERTModels(config_file_path='config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9aae8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': 'Mental_Health_FAQ.csv',\n",
       " 'target_dir': '/Users/satishsilveri/Documents/ML/Search/BEIR_RES',\n",
       " 'models': ['thenlper/gte-large',\n",
       "  'BAAI/bge-large-en-v1.5',\n",
       "  'intfloat/e5-large-v2'],\n",
       " 'batch_size': 100,\n",
       " 'score_function': 'cos_sim',\n",
       " 'question_column': 'Questions',\n",
       " 'answer_column': 'Answers',\n",
       " 'negative_samples': True,\n",
       " 'negative_sampler_model_id': 'random',\n",
       " 'negative_sample_size': 10,\n",
       " 'threshold_sample_fraction': 0.5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_evaluator.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2596af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c602f4ee9e4f437084159c4ab9a24fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37f535eb5fb4739ad6de144a8b21a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.557099461555481\n",
      "Max: 0.8427419066429138\n",
      "Threshold: 0.6523136099179586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f95260ce0b4d5b99f1f51615910136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd73ba5da1ae4db686f793fd0cb45777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a9e08befa4431dae5bb7f695e100f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f9fe9928a54eeea129c69c122602a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eae9db2d69439ba50ddf8f5107ac16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324c699037a14609847020ecb607e273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17973e90cd6648159b6d81aeef3183fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a818555d9bc4dd2bd30626a5d17d28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbert_evaluator.run_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f67c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
